{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Practice Lab Assignment 1 -Neural Network Implementation from Scratch**"
      ],
      "metadata": {
        "id": "hiDE-Z-6zjxv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Objective:Implement a simple feedforward neural network from scratch in Python\n",
        "without using any in-built deep learning libraries. This implementation will\n",
        "focus on basic components like forward pass, backward propagation(back-propagation), and training using gradient descent.\n"
      ],
      "metadata": {
        "id": "uURDwcxWzZRd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Submitted By:**\n",
        "\n",
        "*   **Name : Khushi Narad**\n",
        "*   **Roll No : 36**\n",
        "\n",
        "*   **PRN No : 202201040084**"
      ],
      "metadata": {
        "id": "KdOjweIDy7vr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Detailed Comments Explanation:**\n",
        "\n",
        "**Sigmoid and Derivative:**\n",
        "\n",
        "sigmoid(x) computes the output of the sigmoid activation function. It maps any real-valued input into a value between 0 and 1.\n",
        "sigmoid_derivative(x) is the derivative of the sigmoid function, which is used during backpropagation to calculate gradients.\n",
        "**Neural Network Class:**\n",
        "\n",
        "The NeuralNetwork class contains the methods to initialize the network, perform the forward and backward passes, and train the network.\n",
        "**Forward Pass:**\n",
        "\n",
        "The forward pass computes the activations of the neurons in the network.\n",
        "The input is passed through the layers, with the weights and biases applied, followed by the activation function (sigmoid) to compute the output.\n",
        "Backward Pass (Backpropagation):\n",
        "\n",
        "During the backward pass, the weights are updated by calculating the error between the predicted output and the true output.\n",
        "The gradients are calculated using the derivative of the sigmoid function, and the weights and biases are updated using gradient descent with a specified learning rate.\n",
        "Training Method:\n",
        "\n",
        "The network is trained by performing multiple epochs, where each epoch involves a forward pass followed by a backward pass.\n",
        "Every 1000 epochs, the loss (mean squared error) is printed to track the network's progress in learning.\n",
        "Main Program:\n",
        "\n",
        "The main program defines a simple XOR dataset, where the inputs are 0 and 1 combinations, and the output is their XOR result.\n",
        "The network is created with 2 input neurons, 4 hidden neurons, and 1 output neuron.\n",
        "The network is trained on the XOR data for 10,000 epochs with a learning rate of 0.1.\n",
        "Output:\n",
        "\n",
        "After training, the network is tested on the same XOR inputs, and the predictions are printed.\n"
      ],
      "metadata": {
        "id": "zX4arqMCywJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sigmoid Activation Function\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Sigmoid activation function.\n",
        "    It maps any input to a value between 0 and 1.\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Derivative of Sigmoid Activation Function\n",
        "def sigmoid_derivative(x):\n",
        "    \"\"\"\n",
        "    Derivative of the sigmoid function.\n",
        "    This is used during backpropagation to calculate the gradient.\n",
        "    \"\"\"\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Neural Network Class Definition\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        \"\"\"\n",
        "        Initialize the neural network with the given sizes for the input, hidden, and output layers.\n",
        "\n",
        "        - input_size: Number of input features\n",
        "        - hidden_size: Number of neurons in the hidden layer\n",
        "        - output_size: Number of output neurons\n",
        "        \"\"\"\n",
        "        # Randomly initialize the weights and biases\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Weights and biases initialization with random values\n",
        "        self.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size)  # Weights from input to hidden layer\n",
        "        self.bias_hidden = np.random.randn(1, self.hidden_size)  # Bias for hidden layer\n",
        "\n",
        "        self.weights_hidden_output = np.random.randn(self.hidden_size, self.output_size)  # Weights from hidden to output layer\n",
        "        self.bias_output = np.random.randn(1, self.output_size)  # Bias for output layer\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Perform the forward pass of the neural network.\n",
        "        Compute the activations for the input, hidden, and output layers.\n",
        "\n",
        "        - X: Input data (features)\n",
        "\n",
        "        Returns the output of the network.\n",
        "        \"\"\"\n",
        "        self.input_layer = X  # Store the input data\n",
        "\n",
        "        # Calculate the input to the hidden layer and apply the activation function\n",
        "        self.hidden_layer_input = np.dot(self.input_layer, self.weights_input_hidden) + self.bias_hidden\n",
        "        self.hidden_layer_output = sigmoid(self.hidden_layer_input)\n",
        "\n",
        "        # Calculate the input to the output layer and apply the activation function\n",
        "        self.output_layer_input = np.dot(self.hidden_layer_output, self.weights_hidden_output) + self.bias_output\n",
        "        self.output_layer_output = sigmoid(self.output_layer_input)\n",
        "\n",
        "        return self.output_layer_output\n",
        "\n",
        "    def backward(self, X, y, learning_rate):\n",
        "        \"\"\"\n",
        "        Perform the backward pass of the neural network (backpropagation).\n",
        "        This step adjusts the weights based on the error in the output.\n",
        "\n",
        "        - X: Input data (features)\n",
        "        - y: True labels (targets)\n",
        "        - learning_rate: The rate at which the weights are adjusted\n",
        "        \"\"\"\n",
        "        # Compute the error in the output layer\n",
        "        error_output = y - self.output_layer_output\n",
        "\n",
        "        # Calculate the gradient (delta) for the output layer\n",
        "        output_layer_delta = error_output * sigmoid_derivative(self.output_layer_output)\n",
        "\n",
        "        # Compute the error in the hidden layer\n",
        "        error_hidden = output_layer_delta.dot(self.weights_hidden_output.T)\n",
        "\n",
        "        # Calculate the gradient (delta) for the hidden layer\n",
        "        hidden_layer_delta = error_hidden * sigmoid_derivative(self.hidden_layer_output)\n",
        "\n",
        "        # Update the weights and biases using the computed gradients\n",
        "        # Update weights from hidden to output layer\n",
        "        self.weights_hidden_output += self.hidden_layer_output.T.dot(output_layer_delta) * learning_rate\n",
        "\n",
        "        # Update bias for the output layer\n",
        "        self.bias_output += np.sum(output_layer_delta, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "        # Update weights from input to hidden layer\n",
        "        self.weights_input_hidden += X.T.dot(hidden_layer_delta) * learning_rate\n",
        "\n",
        "        # Update bias for the hidden layer\n",
        "        self.bias_hidden += np.sum(hidden_layer_delta, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        \"\"\"\n",
        "        Train the neural network on the provided data using the forward and backward passes.\n",
        "\n",
        "        - X: Input data (features)\n",
        "        - y: True labels (targets)\n",
        "        - epochs: Number of times to iterate through the entire dataset\n",
        "        - learning_rate: Rate at which the weights are adjusted\n",
        "        \"\"\"\n",
        "        for epoch in range(epochs):\n",
        "            # Perform a forward pass\n",
        "            self.forward(X)\n",
        "\n",
        "            # Perform a backward pass (backpropagation)\n",
        "            self.backward(X, y, learning_rate)\n",
        "\n",
        "            # Print loss (mean squared error) every 1000 epochs\n",
        "            if epoch % 1000 == 0:\n",
        "                loss = np.mean(np.square(y - self.output_layer_output))  # Mean squared error\n",
        "                print(f\"Epoch {epoch} - Loss: {loss}\")\n",
        "\n",
        "# Main Program\n",
        "if __name__ == \"__main__\":\n",
        "    # Define the XOR problem as a simple example\n",
        "    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Input data (XOR inputs)\n",
        "    y = np.array([[0], [1], [1], [0]])  # Expected output data (XOR outputs)\n",
        "\n",
        "    # Create an instance of the NeuralNetwork class with:\n",
        "    # 2 input neurons, 4 hidden neurons, and 1 output neuron\n",
        "    nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=1)\n",
        "\n",
        "    # Train the network with the XOR dataset for 50,000 epochs and a learning rate of 0.1\n",
        "    print(\"Training the neural network...\")\n",
        "    nn.train(X, y, epochs=50000, learning_rate=0.1)\n",
        "\n",
        "    # After training, print the final predictions of the network\n",
        "    print(\"\\nPredictions after training:\")\n",
        "    print(nn.forward(X))  # Test the network on the XOR inputs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eh0_i_sgwIjy",
        "outputId": "9f238352-fdbf-42a6-cad1-584d7eb178c2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the neural network...\n",
            "Epoch 0 - Loss: 0.29723824602293786\n",
            "Epoch 1000 - Loss: 0.2447848965338119\n",
            "Epoch 2000 - Loss: 0.20439600644432487\n",
            "Epoch 3000 - Loss: 0.10521991667670194\n",
            "Epoch 4000 - Loss: 0.026508670339751163\n",
            "Epoch 5000 - Loss: 0.011608074737847203\n",
            "Epoch 6000 - Loss: 0.006973979171878907\n",
            "Epoch 7000 - Loss: 0.004868175516197261\n",
            "Epoch 8000 - Loss: 0.0036967431701032034\n",
            "Epoch 9000 - Loss: 0.0029605458484728737\n",
            "Epoch 10000 - Loss: 0.0024588863769596553\n",
            "Epoch 11000 - Loss: 0.0020968597449059576\n",
            "Epoch 12000 - Loss: 0.0018241997565190645\n",
            "Epoch 13000 - Loss: 0.0016119590229270796\n",
            "Epoch 14000 - Loss: 0.001442360015353052\n",
            "Epoch 15000 - Loss: 0.0013039142269376308\n",
            "Epoch 16000 - Loss: 0.0011888837742104479\n",
            "Epoch 17000 - Loss: 0.001091875336685691\n",
            "Epoch 18000 - Loss: 0.0010090198303949003\n",
            "Epoch 19000 - Loss: 0.0009374725400542772\n",
            "Epoch 20000 - Loss: 0.0008750970494965649\n",
            "Epoch 21000 - Loss: 0.0008202589243261986\n",
            "Epoch 22000 - Loss: 0.000771687253861433\n",
            "Epoch 23000 - Loss: 0.000728379445259642\n",
            "Epoch 24000 - Loss: 0.0006895343338198135\n",
            "Epoch 25000 - Loss: 0.0006545042770106414\n",
            "Epoch 26000 - Loss: 0.0006227602487176577\n",
            "Epoch 27000 - Loss: 0.0005938660077548018\n",
            "Epoch 28000 - Loss: 0.0005674587105526142\n",
            "Epoch 29000 - Loss: 0.0005432341725538956\n",
            "Epoch 30000 - Loss: 0.000520935531415972\n",
            "Epoch 31000 - Loss: 0.0005003444324083077\n",
            "Epoch 32000 - Loss: 0.00048127410650900996\n",
            "Epoch 33000 - Loss: 0.0004635638846960564\n",
            "Epoch 34000 - Loss: 0.0004470748133076206\n",
            "Epoch 35000 - Loss: 0.0004316861216475483\n",
            "Epoch 36000 - Loss: 0.00041729235513341295\n",
            "Epoch 37000 - Loss: 0.00040380103251524827\n",
            "Epoch 38000 - Loss: 0.0003911307189793052\n",
            "Epoch 39000 - Loss: 0.0003792094316931037\n",
            "Epoch 40000 - Loss: 0.0003679733129109148\n",
            "Epoch 41000 - Loss: 0.0003573655198098409\n",
            "Epoch 42000 - Loss: 0.0003473352909495284\n",
            "Epoch 43000 - Loss: 0.0003378371574962541\n",
            "Epoch 44000 - Loss: 0.00032883027374238917\n",
            "Epoch 45000 - Loss: 0.00032027784643816223\n",
            "Epoch 46000 - Loss: 0.00031214664636833176\n",
            "Epoch 47000 - Loss: 0.0003044065887008963\n",
            "Epoch 48000 - Loss: 0.00029703037109502623\n",
            "Epoch 49000 - Loss: 0.00028999316052221585\n",
            "\n",
            "Predictions after training:\n",
            "[[0.00839404]\n",
            " [0.98414382]\n",
            " [0.98374895]\n",
            " [0.02339048]]\n"
          ]
        }
      ]
    }
  ]
}